/* This file is autogenerated by tracetool, do not edit. */

#ifndef TRACE_ACCEL_KVM_GENERATED_TRACERS_H
#define TRACE_ACCEL_KVM_GENERATED_TRACERS_H

#include "trace/control.h"

extern TraceEvent _TRACE_KVM_IOCTL_EVENT;
extern TraceEvent _TRACE_KVM_VM_IOCTL_EVENT;
extern TraceEvent _TRACE_KVM_VCPU_IOCTL_EVENT;
extern TraceEvent _TRACE_KVM_RUN_EXIT_EVENT;
extern TraceEvent _TRACE_KVM_DEVICE_IOCTL_EVENT;
extern TraceEvent _TRACE_KVM_FAILED_REG_GET_EVENT;
extern TraceEvent _TRACE_KVM_FAILED_REG_SET_EVENT;
extern TraceEvent _TRACE_KVM_INIT_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_CREATE_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_DESTROY_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_PARK_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_UNPARK_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_IRQCHIP_COMMIT_ROUTES_EVENT;
extern TraceEvent _TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE_EVENT;
extern TraceEvent _TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE_EVENT;
extern TraceEvent _TRACE_KVM_IRQCHIP_RELEASE_VIRQ_EVENT;
extern TraceEvent _TRACE_KVM_SET_IOEVENTFD_MMIO_EVENT;
extern TraceEvent _TRACE_KVM_SET_IOEVENTFD_PIO_EVENT;
extern TraceEvent _TRACE_KVM_SET_USER_MEMORY_EVENT;
extern TraceEvent _TRACE_KVM_CLEAR_DIRTY_LOG_EVENT;
extern TraceEvent _TRACE_KVM_RESAMPLE_FD_NOTIFY_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_FULL_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_REAP_VCPU_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_PAGE_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_REAPER_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_REAP_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_REAPER_KICK_EVENT;
extern TraceEvent _TRACE_KVM_DIRTY_RING_FLUSH_EVENT;
extern TraceEvent _TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE_EVENT;
extern TraceEvent _TRACE_KVM_CPU_EXEC_EVENT;
extern TraceEvent _TRACE_KVM_INTERRUPT_EXIT_REQUEST_EVENT;
extern TraceEvent _TRACE_KVM_IO_WINDOW_EXIT_EVENT;
extern TraceEvent _TRACE_KVM_RUN_EXIT_SYSTEM_EVENT_EVENT;
extern TraceEvent _TRACE_KVM_CONVERT_MEMORY_EVENT;
extern TraceEvent _TRACE_KVM_MEMORY_FAULT_EVENT;
extern TraceEvent _TRACE_KVM_SLOTS_GROW_EVENT;
extern uint16_t _TRACE_KVM_IOCTL_DSTATE;
extern uint16_t _TRACE_KVM_VM_IOCTL_DSTATE;
extern uint16_t _TRACE_KVM_VCPU_IOCTL_DSTATE;
extern uint16_t _TRACE_KVM_RUN_EXIT_DSTATE;
extern uint16_t _TRACE_KVM_DEVICE_IOCTL_DSTATE;
extern uint16_t _TRACE_KVM_FAILED_REG_GET_DSTATE;
extern uint16_t _TRACE_KVM_FAILED_REG_SET_DSTATE;
extern uint16_t _TRACE_KVM_INIT_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_CREATE_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_DESTROY_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_PARK_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_UNPARK_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_IRQCHIP_COMMIT_ROUTES_DSTATE;
extern uint16_t _TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE_DSTATE;
extern uint16_t _TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE_DSTATE;
extern uint16_t _TRACE_KVM_IRQCHIP_RELEASE_VIRQ_DSTATE;
extern uint16_t _TRACE_KVM_SET_IOEVENTFD_MMIO_DSTATE;
extern uint16_t _TRACE_KVM_SET_IOEVENTFD_PIO_DSTATE;
extern uint16_t _TRACE_KVM_SET_USER_MEMORY_DSTATE;
extern uint16_t _TRACE_KVM_CLEAR_DIRTY_LOG_DSTATE;
extern uint16_t _TRACE_KVM_RESAMPLE_FD_NOTIFY_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_FULL_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_REAP_VCPU_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_PAGE_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_REAPER_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_REAP_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_REAPER_KICK_DSTATE;
extern uint16_t _TRACE_KVM_DIRTY_RING_FLUSH_DSTATE;
extern uint16_t _TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE_DSTATE;
extern uint16_t _TRACE_KVM_CPU_EXEC_DSTATE;
extern uint16_t _TRACE_KVM_INTERRUPT_EXIT_REQUEST_DSTATE;
extern uint16_t _TRACE_KVM_IO_WINDOW_EXIT_DSTATE;
extern uint16_t _TRACE_KVM_RUN_EXIT_SYSTEM_EVENT_DSTATE;
extern uint16_t _TRACE_KVM_CONVERT_MEMORY_DSTATE;
extern uint16_t _TRACE_KVM_MEMORY_FAULT_DSTATE;
extern uint16_t _TRACE_KVM_SLOTS_GROW_DSTATE;
#define TRACE_KVM_IOCTL_ENABLED 1
#define TRACE_KVM_VM_IOCTL_ENABLED 1
#define TRACE_KVM_VCPU_IOCTL_ENABLED 1
#define TRACE_KVM_RUN_EXIT_ENABLED 1
#define TRACE_KVM_DEVICE_IOCTL_ENABLED 1
#define TRACE_KVM_FAILED_REG_GET_ENABLED 1
#define TRACE_KVM_FAILED_REG_SET_ENABLED 1
#define TRACE_KVM_INIT_VCPU_ENABLED 1
#define TRACE_KVM_CREATE_VCPU_ENABLED 1
#define TRACE_KVM_DESTROY_VCPU_ENABLED 1
#define TRACE_KVM_PARK_VCPU_ENABLED 1
#define TRACE_KVM_UNPARK_VCPU_ENABLED 1
#define TRACE_KVM_IRQCHIP_COMMIT_ROUTES_ENABLED 1
#define TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE_ENABLED 1
#define TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE_ENABLED 1
#define TRACE_KVM_IRQCHIP_RELEASE_VIRQ_ENABLED 1
#define TRACE_KVM_SET_IOEVENTFD_MMIO_ENABLED 1
#define TRACE_KVM_SET_IOEVENTFD_PIO_ENABLED 1
#define TRACE_KVM_SET_USER_MEMORY_ENABLED 1
#define TRACE_KVM_CLEAR_DIRTY_LOG_ENABLED 1
#define TRACE_KVM_RESAMPLE_FD_NOTIFY_ENABLED 1
#define TRACE_KVM_DIRTY_RING_FULL_ENABLED 1
#define TRACE_KVM_DIRTY_RING_REAP_VCPU_ENABLED 1
#define TRACE_KVM_DIRTY_RING_PAGE_ENABLED 1
#define TRACE_KVM_DIRTY_RING_REAPER_ENABLED 1
#define TRACE_KVM_DIRTY_RING_REAP_ENABLED 1
#define TRACE_KVM_DIRTY_RING_REAPER_KICK_ENABLED 1
#define TRACE_KVM_DIRTY_RING_FLUSH_ENABLED 1
#define TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE_ENABLED 1
#define TRACE_KVM_CPU_EXEC_ENABLED 1
#define TRACE_KVM_INTERRUPT_EXIT_REQUEST_ENABLED 1
#define TRACE_KVM_IO_WINDOW_EXIT_ENABLED 1
#define TRACE_KVM_RUN_EXIT_SYSTEM_EVENT_ENABLED 1
#define TRACE_KVM_CONVERT_MEMORY_ENABLED 1
#define TRACE_KVM_MEMORY_FAULT_ENABLED 1
#define TRACE_KVM_SLOTS_GROW_ENABLED 1
#include "qemu/log-for-trace.h"


#define TRACE_KVM_IOCTL_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IOCTL) || \
    false)

static inline void _nocheck__trace_kvm_ioctl(unsigned long type, void * arg)
{
    if (trace_event_get_state(TRACE_KVM_IOCTL) && qemu_loglevel_mask(LOG_TRACE)) {
#line 4 "../accel/kvm/trace-events"
        qemu_log("kvm_ioctl " "type 0x%lx, arg %p" "\n", type, arg);
#line 129 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_ioctl(unsigned long type, void * arg)
{
    if (true) {
        _nocheck__trace_kvm_ioctl(type, arg);
    }
}

#define TRACE_KVM_VM_IOCTL_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_VM_IOCTL) || \
    false)

static inline void _nocheck__trace_kvm_vm_ioctl(unsigned long type, void * arg)
{
    if (trace_event_get_state(TRACE_KVM_VM_IOCTL) && qemu_loglevel_mask(LOG_TRACE)) {
#line 5 "../accel/kvm/trace-events"
        qemu_log("kvm_vm_ioctl " "type 0x%lx, arg %p" "\n", type, arg);
#line 149 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_vm_ioctl(unsigned long type, void * arg)
{
    if (true) {
        _nocheck__trace_kvm_vm_ioctl(type, arg);
    }
}

#define TRACE_KVM_VCPU_IOCTL_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_VCPU_IOCTL) || \
    false)

static inline void _nocheck__trace_kvm_vcpu_ioctl(int cpu_index, unsigned long type, void * arg)
{
    if (trace_event_get_state(TRACE_KVM_VCPU_IOCTL) && qemu_loglevel_mask(LOG_TRACE)) {
#line 6 "../accel/kvm/trace-events"
        qemu_log("kvm_vcpu_ioctl " "cpu_index %d, type 0x%lx, arg %p" "\n", cpu_index, type, arg);
#line 169 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_vcpu_ioctl(int cpu_index, unsigned long type, void * arg)
{
    if (true) {
        _nocheck__trace_kvm_vcpu_ioctl(cpu_index, type, arg);
    }
}

#define TRACE_KVM_RUN_EXIT_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_RUN_EXIT) || \
    false)

static inline void _nocheck__trace_kvm_run_exit(int cpu_index, uint32_t reason)
{
    if (trace_event_get_state(TRACE_KVM_RUN_EXIT) && qemu_loglevel_mask(LOG_TRACE)) {
#line 7 "../accel/kvm/trace-events"
        qemu_log("kvm_run_exit " "cpu_index %d, reason %d" "\n", cpu_index, reason);
#line 189 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_run_exit(int cpu_index, uint32_t reason)
{
    if (true) {
        _nocheck__trace_kvm_run_exit(cpu_index, reason);
    }
}

#define TRACE_KVM_DEVICE_IOCTL_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DEVICE_IOCTL) || \
    false)

static inline void _nocheck__trace_kvm_device_ioctl(int fd, unsigned long type, void * arg)
{
    if (trace_event_get_state(TRACE_KVM_DEVICE_IOCTL) && qemu_loglevel_mask(LOG_TRACE)) {
#line 8 "../accel/kvm/trace-events"
        qemu_log("kvm_device_ioctl " "dev fd %d, type 0x%lx, arg %p" "\n", fd, type, arg);
#line 209 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_device_ioctl(int fd, unsigned long type, void * arg)
{
    if (true) {
        _nocheck__trace_kvm_device_ioctl(fd, type, arg);
    }
}

#define TRACE_KVM_FAILED_REG_GET_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_FAILED_REG_GET) || \
    false)

static inline void _nocheck__trace_kvm_failed_reg_get(uint64_t id, const char * msg)
{
    if (trace_event_get_state(TRACE_KVM_FAILED_REG_GET) && qemu_loglevel_mask(LOG_TRACE)) {
#line 9 "../accel/kvm/trace-events"
        qemu_log("kvm_failed_reg_get " "Warning: Unable to retrieve ONEREG %" PRIu64 " from KVM: %s" "\n", id, msg);
#line 229 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_failed_reg_get(uint64_t id, const char * msg)
{
    if (true) {
        _nocheck__trace_kvm_failed_reg_get(id, msg);
    }
}

#define TRACE_KVM_FAILED_REG_SET_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_FAILED_REG_SET) || \
    false)

static inline void _nocheck__trace_kvm_failed_reg_set(uint64_t id, const char * msg)
{
    if (trace_event_get_state(TRACE_KVM_FAILED_REG_SET) && qemu_loglevel_mask(LOG_TRACE)) {
#line 10 "../accel/kvm/trace-events"
        qemu_log("kvm_failed_reg_set " "Warning: Unable to set ONEREG %" PRIu64 " to KVM: %s" "\n", id, msg);
#line 249 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_failed_reg_set(uint64_t id, const char * msg)
{
    if (true) {
        _nocheck__trace_kvm_failed_reg_set(id, msg);
    }
}

#define TRACE_KVM_INIT_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_INIT_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_init_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (trace_event_get_state(TRACE_KVM_INIT_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 11 "../accel/kvm/trace-events"
        qemu_log("kvm_init_vcpu " "index: %d id: %lu" "\n", cpu_index, arch_cpu_id);
#line 269 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_init_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (true) {
        _nocheck__trace_kvm_init_vcpu(cpu_index, arch_cpu_id);
    }
}

#define TRACE_KVM_CREATE_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_CREATE_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_create_vcpu(int cpu_index, unsigned long arch_cpu_id, int kvm_fd)
{
    if (trace_event_get_state(TRACE_KVM_CREATE_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 12 "../accel/kvm/trace-events"
        qemu_log("kvm_create_vcpu " "index: %d, id: %lu, kvm fd: %d" "\n", cpu_index, arch_cpu_id, kvm_fd);
#line 289 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_create_vcpu(int cpu_index, unsigned long arch_cpu_id, int kvm_fd)
{
    if (true) {
        _nocheck__trace_kvm_create_vcpu(cpu_index, arch_cpu_id, kvm_fd);
    }
}

#define TRACE_KVM_DESTROY_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DESTROY_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_destroy_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (trace_event_get_state(TRACE_KVM_DESTROY_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 13 "../accel/kvm/trace-events"
        qemu_log("kvm_destroy_vcpu " "index: %d id: %lu" "\n", cpu_index, arch_cpu_id);
#line 309 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_destroy_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (true) {
        _nocheck__trace_kvm_destroy_vcpu(cpu_index, arch_cpu_id);
    }
}

#define TRACE_KVM_PARK_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_PARK_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_park_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (trace_event_get_state(TRACE_KVM_PARK_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 14 "../accel/kvm/trace-events"
        qemu_log("kvm_park_vcpu " "index: %d id: %lu" "\n", cpu_index, arch_cpu_id);
#line 329 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_park_vcpu(int cpu_index, unsigned long arch_cpu_id)
{
    if (true) {
        _nocheck__trace_kvm_park_vcpu(cpu_index, arch_cpu_id);
    }
}

#define TRACE_KVM_UNPARK_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_UNPARK_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_unpark_vcpu(unsigned long arch_cpu_id, const char * msg)
{
    if (trace_event_get_state(TRACE_KVM_UNPARK_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 15 "../accel/kvm/trace-events"
        qemu_log("kvm_unpark_vcpu " "id: %lu %s" "\n", arch_cpu_id, msg);
#line 349 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_unpark_vcpu(unsigned long arch_cpu_id, const char * msg)
{
    if (true) {
        _nocheck__trace_kvm_unpark_vcpu(arch_cpu_id, msg);
    }
}

#define TRACE_KVM_IRQCHIP_COMMIT_ROUTES_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IRQCHIP_COMMIT_ROUTES) || \
    false)

static inline void _nocheck__trace_kvm_irqchip_commit_routes(void)
{
    if (trace_event_get_state(TRACE_KVM_IRQCHIP_COMMIT_ROUTES) && qemu_loglevel_mask(LOG_TRACE)) {
#line 16 "../accel/kvm/trace-events"
        qemu_log("kvm_irqchip_commit_routes " "" "\n");
#line 369 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_irqchip_commit_routes(void)
{
    if (true) {
        _nocheck__trace_kvm_irqchip_commit_routes();
    }
}

#define TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE) || \
    false)

static inline void _nocheck__trace_kvm_irqchip_add_msi_route(char * name, int vector, int virq)
{
    if (trace_event_get_state(TRACE_KVM_IRQCHIP_ADD_MSI_ROUTE) && qemu_loglevel_mask(LOG_TRACE)) {
#line 17 "../accel/kvm/trace-events"
        qemu_log("kvm_irqchip_add_msi_route " "dev %s vector %d virq %d" "\n", name, vector, virq);
#line 389 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_irqchip_add_msi_route(char * name, int vector, int virq)
{
    if (true) {
        _nocheck__trace_kvm_irqchip_add_msi_route(name, vector, virq);
    }
}

#define TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE) || \
    false)

static inline void _nocheck__trace_kvm_irqchip_update_msi_route(int virq)
{
    if (trace_event_get_state(TRACE_KVM_IRQCHIP_UPDATE_MSI_ROUTE) && qemu_loglevel_mask(LOG_TRACE)) {
#line 18 "../accel/kvm/trace-events"
        qemu_log("kvm_irqchip_update_msi_route " "Updating MSI route virq=%d" "\n", virq);
#line 409 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_irqchip_update_msi_route(int virq)
{
    if (true) {
        _nocheck__trace_kvm_irqchip_update_msi_route(virq);
    }
}

#define TRACE_KVM_IRQCHIP_RELEASE_VIRQ_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IRQCHIP_RELEASE_VIRQ) || \
    false)

static inline void _nocheck__trace_kvm_irqchip_release_virq(int virq)
{
    if (trace_event_get_state(TRACE_KVM_IRQCHIP_RELEASE_VIRQ) && qemu_loglevel_mask(LOG_TRACE)) {
#line 19 "../accel/kvm/trace-events"
        qemu_log("kvm_irqchip_release_virq " "virq %d" "\n", virq);
#line 429 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_irqchip_release_virq(int virq)
{
    if (true) {
        _nocheck__trace_kvm_irqchip_release_virq(virq);
    }
}

#define TRACE_KVM_SET_IOEVENTFD_MMIO_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_SET_IOEVENTFD_MMIO) || \
    false)

static inline void _nocheck__trace_kvm_set_ioeventfd_mmio(int fd, uint64_t addr, uint32_t val, bool assign, uint32_t size, bool datamatch)
{
    if (trace_event_get_state(TRACE_KVM_SET_IOEVENTFD_MMIO) && qemu_loglevel_mask(LOG_TRACE)) {
#line 20 "../accel/kvm/trace-events"
        qemu_log("kvm_set_ioeventfd_mmio " "fd: %d @0x%" PRIx64 " val=0x%x assign: %d size: %d match: %d" "\n", fd, addr, val, assign, size, datamatch);
#line 449 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_set_ioeventfd_mmio(int fd, uint64_t addr, uint32_t val, bool assign, uint32_t size, bool datamatch)
{
    if (true) {
        _nocheck__trace_kvm_set_ioeventfd_mmio(fd, addr, val, assign, size, datamatch);
    }
}

#define TRACE_KVM_SET_IOEVENTFD_PIO_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_SET_IOEVENTFD_PIO) || \
    false)

static inline void _nocheck__trace_kvm_set_ioeventfd_pio(int fd, uint16_t addr, uint32_t val, bool assign, uint32_t size, bool datamatch)
{
    if (trace_event_get_state(TRACE_KVM_SET_IOEVENTFD_PIO) && qemu_loglevel_mask(LOG_TRACE)) {
#line 21 "../accel/kvm/trace-events"
        qemu_log("kvm_set_ioeventfd_pio " "fd: %d @0x%x val=0x%x assign: %d size: %d match: %d" "\n", fd, addr, val, assign, size, datamatch);
#line 469 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_set_ioeventfd_pio(int fd, uint16_t addr, uint32_t val, bool assign, uint32_t size, bool datamatch)
{
    if (true) {
        _nocheck__trace_kvm_set_ioeventfd_pio(fd, addr, val, assign, size, datamatch);
    }
}

#define TRACE_KVM_SET_USER_MEMORY_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_SET_USER_MEMORY) || \
    false)

static inline void _nocheck__trace_kvm_set_user_memory(uint16_t as, uint16_t slot, uint32_t flags, uint64_t guest_phys_addr, uint64_t memory_size, uint64_t userspace_addr, uint32_t fd, uint64_t fd_offset, int ret)
{
    if (trace_event_get_state(TRACE_KVM_SET_USER_MEMORY) && qemu_loglevel_mask(LOG_TRACE)) {
#line 22 "../accel/kvm/trace-events"
        qemu_log("kvm_set_user_memory " "AddrSpace#%d Slot#%d flags=0x%x gpa=0x%"PRIx64 " size=0x%"PRIx64 " ua=0x%"PRIx64 " guest_memfd=%d" " guest_memfd_offset=0x%" PRIx64 " ret=%d" "\n", as, slot, flags, guest_phys_addr, memory_size, userspace_addr, fd, fd_offset, ret);
#line 489 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_set_user_memory(uint16_t as, uint16_t slot, uint32_t flags, uint64_t guest_phys_addr, uint64_t memory_size, uint64_t userspace_addr, uint32_t fd, uint64_t fd_offset, int ret)
{
    if (true) {
        _nocheck__trace_kvm_set_user_memory(as, slot, flags, guest_phys_addr, memory_size, userspace_addr, fd, fd_offset, ret);
    }
}

#define TRACE_KVM_CLEAR_DIRTY_LOG_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_CLEAR_DIRTY_LOG) || \
    false)

static inline void _nocheck__trace_kvm_clear_dirty_log(uint32_t slot, uint64_t start, uint32_t size)
{
    if (trace_event_get_state(TRACE_KVM_CLEAR_DIRTY_LOG) && qemu_loglevel_mask(LOG_TRACE)) {
#line 23 "../accel/kvm/trace-events"
        qemu_log("kvm_clear_dirty_log " "slot#%"PRId32" start 0x%"PRIx64" size 0x%"PRIx32 "\n", slot, start, size);
#line 509 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_clear_dirty_log(uint32_t slot, uint64_t start, uint32_t size)
{
    if (true) {
        _nocheck__trace_kvm_clear_dirty_log(slot, start, size);
    }
}

#define TRACE_KVM_RESAMPLE_FD_NOTIFY_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_RESAMPLE_FD_NOTIFY) || \
    false)

static inline void _nocheck__trace_kvm_resample_fd_notify(int gsi)
{
    if (trace_event_get_state(TRACE_KVM_RESAMPLE_FD_NOTIFY) && qemu_loglevel_mask(LOG_TRACE)) {
#line 24 "../accel/kvm/trace-events"
        qemu_log("kvm_resample_fd_notify " "gsi %d" "\n", gsi);
#line 529 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_resample_fd_notify(int gsi)
{
    if (true) {
        _nocheck__trace_kvm_resample_fd_notify(gsi);
    }
}

#define TRACE_KVM_DIRTY_RING_FULL_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_FULL) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_full(int id)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_FULL) && qemu_loglevel_mask(LOG_TRACE)) {
#line 25 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_full " "vcpu %d" "\n", id);
#line 549 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_full(int id)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_full(id);
    }
}

#define TRACE_KVM_DIRTY_RING_REAP_VCPU_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_REAP_VCPU) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_reap_vcpu(int id)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_REAP_VCPU) && qemu_loglevel_mask(LOG_TRACE)) {
#line 26 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_reap_vcpu " "vcpu %d" "\n", id);
#line 569 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_reap_vcpu(int id)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_reap_vcpu(id);
    }
}

#define TRACE_KVM_DIRTY_RING_PAGE_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_PAGE) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_page(int vcpu, uint32_t slot, uint64_t offset)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_PAGE) && qemu_loglevel_mask(LOG_TRACE)) {
#line 27 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_page " "vcpu %d fetch %"PRIu32" offset 0x%"PRIx64 "\n", vcpu, slot, offset);
#line 589 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_page(int vcpu, uint32_t slot, uint64_t offset)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_page(vcpu, slot, offset);
    }
}

#define TRACE_KVM_DIRTY_RING_REAPER_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_REAPER) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_reaper(const char * s)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_REAPER) && qemu_loglevel_mask(LOG_TRACE)) {
#line 28 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_reaper " "%s" "\n", s);
#line 609 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_reaper(const char * s)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_reaper(s);
    }
}

#define TRACE_KVM_DIRTY_RING_REAP_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_REAP) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_reap(uint64_t count, int64_t t)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_REAP) && qemu_loglevel_mask(LOG_TRACE)) {
#line 29 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_reap " "reaped %"PRIu64" pages (took %"PRIi64" us)" "\n", count, t);
#line 629 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_reap(uint64_t count, int64_t t)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_reap(count, t);
    }
}

#define TRACE_KVM_DIRTY_RING_REAPER_KICK_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_REAPER_KICK) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_reaper_kick(const char * reason)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_REAPER_KICK) && qemu_loglevel_mask(LOG_TRACE)) {
#line 30 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_reaper_kick " "%s" "\n", reason);
#line 649 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_reaper_kick(const char * reason)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_reaper_kick(reason);
    }
}

#define TRACE_KVM_DIRTY_RING_FLUSH_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_DIRTY_RING_FLUSH) || \
    false)

static inline void _nocheck__trace_kvm_dirty_ring_flush(int finished)
{
    if (trace_event_get_state(TRACE_KVM_DIRTY_RING_FLUSH) && qemu_loglevel_mask(LOG_TRACE)) {
#line 31 "../accel/kvm/trace-events"
        qemu_log("kvm_dirty_ring_flush " "%d" "\n", finished);
#line 669 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_dirty_ring_flush(int finished)
{
    if (true) {
        _nocheck__trace_kvm_dirty_ring_flush(finished);
    }
}

#define TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE) || \
    false)

static inline void _nocheck__trace_kvm_failed_get_vcpu_mmap_size(void)
{
    if (trace_event_get_state(TRACE_KVM_FAILED_GET_VCPU_MMAP_SIZE) && qemu_loglevel_mask(LOG_TRACE)) {
#line 32 "../accel/kvm/trace-events"
        qemu_log("kvm_failed_get_vcpu_mmap_size " "" "\n");
#line 689 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_failed_get_vcpu_mmap_size(void)
{
    if (true) {
        _nocheck__trace_kvm_failed_get_vcpu_mmap_size();
    }
}

#define TRACE_KVM_CPU_EXEC_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_CPU_EXEC) || \
    false)

static inline void _nocheck__trace_kvm_cpu_exec(void)
{
    if (trace_event_get_state(TRACE_KVM_CPU_EXEC) && qemu_loglevel_mask(LOG_TRACE)) {
#line 33 "../accel/kvm/trace-events"
        qemu_log("kvm_cpu_exec " "" "\n");
#line 709 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_cpu_exec(void)
{
    if (true) {
        _nocheck__trace_kvm_cpu_exec();
    }
}

#define TRACE_KVM_INTERRUPT_EXIT_REQUEST_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_INTERRUPT_EXIT_REQUEST) || \
    false)

static inline void _nocheck__trace_kvm_interrupt_exit_request(void)
{
    if (trace_event_get_state(TRACE_KVM_INTERRUPT_EXIT_REQUEST) && qemu_loglevel_mask(LOG_TRACE)) {
#line 34 "../accel/kvm/trace-events"
        qemu_log("kvm_interrupt_exit_request " "" "\n");
#line 729 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_interrupt_exit_request(void)
{
    if (true) {
        _nocheck__trace_kvm_interrupt_exit_request();
    }
}

#define TRACE_KVM_IO_WINDOW_EXIT_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_IO_WINDOW_EXIT) || \
    false)

static inline void _nocheck__trace_kvm_io_window_exit(void)
{
    if (trace_event_get_state(TRACE_KVM_IO_WINDOW_EXIT) && qemu_loglevel_mask(LOG_TRACE)) {
#line 35 "../accel/kvm/trace-events"
        qemu_log("kvm_io_window_exit " "" "\n");
#line 749 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_io_window_exit(void)
{
    if (true) {
        _nocheck__trace_kvm_io_window_exit();
    }
}

#define TRACE_KVM_RUN_EXIT_SYSTEM_EVENT_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_RUN_EXIT_SYSTEM_EVENT) || \
    false)

static inline void _nocheck__trace_kvm_run_exit_system_event(int cpu_index, uint32_t event_type)
{
    if (trace_event_get_state(TRACE_KVM_RUN_EXIT_SYSTEM_EVENT) && qemu_loglevel_mask(LOG_TRACE)) {
#line 36 "../accel/kvm/trace-events"
        qemu_log("kvm_run_exit_system_event " "cpu_index %d, system_even_type %"PRIu32 "\n", cpu_index, event_type);
#line 769 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_run_exit_system_event(int cpu_index, uint32_t event_type)
{
    if (true) {
        _nocheck__trace_kvm_run_exit_system_event(cpu_index, event_type);
    }
}

#define TRACE_KVM_CONVERT_MEMORY_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_CONVERT_MEMORY) || \
    false)

static inline void _nocheck__trace_kvm_convert_memory(uint64_t start, uint64_t size, const char * msg)
{
    if (trace_event_get_state(TRACE_KVM_CONVERT_MEMORY) && qemu_loglevel_mask(LOG_TRACE)) {
#line 37 "../accel/kvm/trace-events"
        qemu_log("kvm_convert_memory " "start 0x%" PRIx64 " size 0x%" PRIx64 " %s" "\n", start, size, msg);
#line 789 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_convert_memory(uint64_t start, uint64_t size, const char * msg)
{
    if (true) {
        _nocheck__trace_kvm_convert_memory(start, size, msg);
    }
}

#define TRACE_KVM_MEMORY_FAULT_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_MEMORY_FAULT) || \
    false)

static inline void _nocheck__trace_kvm_memory_fault(uint64_t start, uint64_t size, uint64_t flags)
{
    if (trace_event_get_state(TRACE_KVM_MEMORY_FAULT) && qemu_loglevel_mask(LOG_TRACE)) {
#line 38 "../accel/kvm/trace-events"
        qemu_log("kvm_memory_fault " "start 0x%" PRIx64 " size 0x%" PRIx64 " flags 0x%" PRIx64 "\n", start, size, flags);
#line 809 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_memory_fault(uint64_t start, uint64_t size, uint64_t flags)
{
    if (true) {
        _nocheck__trace_kvm_memory_fault(start, size, flags);
    }
}

#define TRACE_KVM_SLOTS_GROW_BACKEND_DSTATE() ( \
    trace_event_get_state_dynamic_by_id(TRACE_KVM_SLOTS_GROW) || \
    false)

static inline void _nocheck__trace_kvm_slots_grow(unsigned int old, unsigned int new)
{
    if (trace_event_get_state(TRACE_KVM_SLOTS_GROW) && qemu_loglevel_mask(LOG_TRACE)) {
#line 39 "../accel/kvm/trace-events"
        qemu_log("kvm_slots_grow " "%u -> %u" "\n", old, new);
#line 829 "trace/trace-accel_kvm.h"
    }
}

static inline void trace_kvm_slots_grow(unsigned int old, unsigned int new)
{
    if (true) {
        _nocheck__trace_kvm_slots_grow(old, new);
    }
}
#endif /* TRACE_ACCEL_KVM_GENERATED_TRACERS_H */
